# =============================================================================
# Lakehouse Orchestrator - Docker Compose
# =============================================================================
# Enterprise-grade lakehouse platform
# Stack: Airflow (Celery) + Valkey + PostgreSQL + SeaweedFS + Trino + Superset
#
# Usage:
#   cp .env.example .env
#   docker compose up -d
#
# =============================================================================

x-airflow-common: &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-}
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${POSTGRES_DB:-airflow}
    AIRFLOW__CELERY__BROKER_URL: redis://valkey:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${POSTGRES_DB:-airflow}
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY:-change-me}
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "false"
    # S3 / SeaweedFS configuration for DAGs
    S3_ENDPOINT_URL: http://seaweedfs-s3:8333
    S3_BUCKET_NAME: ${S3_BUCKET_NAME:-lakehouse}
    S3_ACCESS_KEY: ${SEAWEEDFS_S3_ACCESS_KEY:-lakehouse_access_key}
    S3_SECRET_KEY: ${SEAWEEDFS_S3_SECRET_KEY:-lakehouse_secret_key}
    S3_REGION: ${S3_REGION:-us-east-1}
    S3_INGEST_BUCKET: ${S3_INGEST_BUCKET:-csv-uploads}
    # Trino connection for DAGs
    TRINO_HOST: trino
    TRINO_PORT: "8080"
    TRINO_CATALOG: iceberg
    TRINO_SCHEMA: lakehouse
    # StatsD metrics → Prometheus
    AIRFLOW__METRICS__STATSD_ON: "true"
    AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
    AIRFLOW__METRICS__STATSD_PORT: "8125"
    AIRFLOW__METRICS__STATSD_PREFIX: airflow
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/plugins:/opt/airflow/plugins
    - ./data:/opt/airflow/data
    - airflow-logs:/opt/airflow/logs
  depends_on:
    postgres:
      condition: service_healthy
    valkey:
      condition: service_healthy
  networks:
    - lakehouse

services:
  # ===========================================================================
  # PostgreSQL - Airflow metadata + Celery result backend + Superset metadata
  # ===========================================================================
  postgres:
    image: postgres:16-alpine
    container_name: lakehouse-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
      # Passed to init script for Superset DB creation
      SUPERSET_DB_USER: ${SUPERSET_DB_USER:-superset}
      SUPERSET_DB_PASSWORD: ${SUPERSET_DB_PASSWORD:-superset}
      SUPERSET_DB_NAME: ${SUPERSET_DB_NAME:-superset}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/init-superset-db.sh:/docker-entrypoint-initdb.d/init-superset-db.sh
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-airflow}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse

  # ===========================================================================
  # Valkey - Celery broker (Redis-compatible, BSD license)
  # ===========================================================================
  valkey:
    image: valkey/valkey:8-alpine
    container_name: lakehouse-valkey
    restart: unless-stopped
    ports:
      - "${VALKEY_PORT:-6379}:6379"
    volumes:
      - valkey-data:/data
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse

  # ===========================================================================
  # SeaweedFS - S3-compatible object storage (Apache 2.0)
  # ===========================================================================
  seaweedfs-master:
    image: chrislusf/seaweedfs:latest
    container_name: lakehouse-seaweedfs-master
    restart: unless-stopped
    command: "master -ip=seaweedfs-master -ip.bind=0.0.0.0 -metricsPort=9324"
    ports:
      - "${SEAWEEDFS_MASTER_PORT:-9333}:9333"
    volumes:
      - seaweedfs-master-data:/data
    networks:
      - lakehouse

  seaweedfs-volume:
    image: chrislusf/seaweedfs:latest
    container_name: lakehouse-seaweedfs-volume
    restart: unless-stopped
    command: "volume -mserver=seaweedfs-master:9333 -ip.bind=0.0.0.0 -port=8080 -metricsPort=9325"
    ports:
      - "${SEAWEEDFS_VOLUME_PORT:-8082}:8080"
    volumes:
      - seaweedfs-volume-data:/data
    depends_on:
      - seaweedfs-master
    networks:
      - lakehouse

  seaweedfs-s3:
    image: chrislusf/seaweedfs:latest
    container_name: lakehouse-seaweedfs-s3
    restart: unless-stopped
    command: "s3 -filer=seaweedfs-filer:8888 -config=/etc/seaweedfs/s3-config.json -ip.bind=0.0.0.0 -port=8333"
    ports:
      - "${SEAWEEDFS_S3_PORT:-8333}:8333"
    volumes:
      - ./seaweedfs/s3-config.json:/etc/seaweedfs/s3-config.json:ro
    depends_on:
      - seaweedfs-filer
    networks:
      - lakehouse

  seaweedfs-filer:
    image: chrislusf/seaweedfs:latest
    container_name: lakehouse-seaweedfs-filer
    restart: unless-stopped
    command: "filer -master=seaweedfs-master:9333 -ip.bind=0.0.0.0"
    ports:
      - "8888:8888"
    volumes:
      - seaweedfs-filer-data:/data
    depends_on:
      - seaweedfs-master
      - seaweedfs-volume
    networks:
      - lakehouse

  # ===========================================================================
  # S3 Bucket Initializer - Creates the lakehouse bucket on startup
  # ===========================================================================
  s3-init:
    image: amazon/aws-cli:latest
    container_name: lakehouse-s3-init
    entrypoint: /bin/sh
    command: >
      -c "
        echo 'Waiting for SeaweedFS S3 gateway...' &&
        sleep 15 &&
        aws --endpoint-url http://seaweedfs-s3:8333 s3 mb s3://${S3_BUCKET_NAME:-lakehouse} --region ${S3_REGION:-us-east-1} 2>/dev/null || true &&
        aws --endpoint-url http://seaweedfs-s3:8333 s3 mb s3://${S3_BUCKET_NAME:-lakehouse}-warehouse --region ${S3_REGION:-us-east-1} 2>/dev/null || true &&
        aws --endpoint-url http://seaweedfs-s3:8333 s3 mb s3://${S3_INGEST_BUCKET:-csv-uploads} --region ${S3_REGION:-us-east-1} 2>/dev/null || true &&
        echo 'Buckets created successfully' &&
        aws --endpoint-url http://seaweedfs-s3:8333 s3 ls
      "
    environment:
      AWS_ACCESS_KEY_ID: ${SEAWEEDFS_S3_ACCESS_KEY:-lakehouse_access_key}
      AWS_SECRET_ACCESS_KEY: ${SEAWEEDFS_S3_SECRET_KEY:-lakehouse_secret_key}
      AWS_DEFAULT_REGION: ${S3_REGION:-us-east-1}
    depends_on:
      - seaweedfs-s3
    networks:
      - lakehouse

  # ===========================================================================
  # Airflow - Webserver
  # ===========================================================================
  airflow-init:
    <<: *airflow-common
    container_name: lakehouse-airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate &&
        airflow users create \
          --username "$${AIRFLOW_ADMIN_USERNAME:-admin}" \
          --password "$${AIRFLOW_ADMIN_PASSWORD:-admin}" \
          --firstname "$${AIRFLOW_ADMIN_FIRSTNAME:-Admin}" \
          --lastname "$${AIRFLOW_ADMIN_LASTNAME:-User}" \
          --role Admin \
          --email "$${AIRFLOW_ADMIN_EMAIL:-admin@lakehouse.local}" || true
    environment:
      <<: *airflow-common-env
      AIRFLOW_ADMIN_USERNAME: ${AIRFLOW_ADMIN_USERNAME:-admin}
      AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
      AIRFLOW_ADMIN_FIRSTNAME: ${AIRFLOW_ADMIN_FIRSTNAME:-Admin}
      AIRFLOW_ADMIN_LASTNAME: ${AIRFLOW_ADMIN_LASTNAME:-User}
      AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL:-admin@lakehouse.local}
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    container_name: lakehouse-airflow-webserver
    command: airflow webserver
    restart: unless-stopped
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: lakehouse-airflow-scheduler
    command: airflow scheduler
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    container_name: lakehouse-airflow-worker
    command: airflow celery worker
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d celery@$${HOSTNAME} || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-flower:
    <<: *airflow-common
    container_name: lakehouse-airflow-flower
    command: airflow celery flower
    restart: unless-stopped
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5555/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # ===========================================================================
  # Iceberg REST Catalog - Metadata service for Iceberg tables (Apache 2.0)
  # ===========================================================================
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: lakehouse-iceberg-rest
    restart: unless-stopped
    ports:
      - "8181:8181"
    environment:
      CATALOG_WAREHOUSE: s3://lakehouse-warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://seaweedfs-s3:8333
      CATALOG_S3_ACCESS__KEY__ID: ${SEAWEEDFS_S3_ACCESS_KEY:-lakehouse_access_key}
      CATALOG_S3_SECRET__ACCESS__KEY: ${SEAWEEDFS_S3_SECRET_KEY:-lakehouse_secret_key}
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      CATALOG_S3_REGION: ${S3_REGION:-us-east-1}
      AWS_ACCESS_KEY_ID: ${SEAWEEDFS_S3_ACCESS_KEY:-lakehouse_access_key}
      AWS_SECRET_ACCESS_KEY: ${SEAWEEDFS_S3_SECRET_KEY:-lakehouse_secret_key}
      AWS_REGION: ${S3_REGION:-us-east-1}
    depends_on:
      - seaweedfs-s3
    networks:
      - lakehouse

  # ===========================================================================
  # Trino - Distributed SQL query engine with Iceberg catalog
  # ===========================================================================
  trino:
    image: trinodb/trino:latest
    container_name: lakehouse-trino
    restart: unless-stopped
    ports:
      - "${TRINO_PORT:-8083}:8080"
    volumes:
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties:ro
      - ./trino/config.properties:/etc/trino/config.properties:ro
    depends_on:
      - seaweedfs-s3
      - iceberg-rest
    networks:
      - lakehouse

  # ===========================================================================
  # Superset - Business Intelligence & Dashboards
  # ===========================================================================
  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    container_name: lakehouse-superset
    restart: unless-stopped
    ports:
      - "${SUPERSET_PORT:-8088}:8088"
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-change-me-to-a-secure-key}
      SUPERSET_ADMIN_USERNAME: ${SUPERSET_ADMIN_USERNAME:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      SUPERSET_ADMIN_FIRSTNAME: ${SUPERSET_ADMIN_FIRSTNAME:-Admin}
      SUPERSET_ADMIN_LASTNAME: ${SUPERSET_ADMIN_LASTNAME:-User}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL:-admin@lakehouse.local}
      SUPERSET_DB_URI: postgresql+psycopg2://${SUPERSET_DB_USER:-superset}:${SUPERSET_DB_PASSWORD:-superset}@postgres:5432/${SUPERSET_DB_NAME:-superset}
      TRINO_HOST: trino
      TRINO_PORT: "8080"
    depends_on:
      postgres:
        condition: service_healthy
      trino:
        condition: service_started
    networks:
      - lakehouse

  # ===========================================================================
  # Monitoring - StatsD Exporter (Airflow StatsD → Prometheus format)
  # ===========================================================================
  statsd-exporter:
    image: prom/statsd-exporter:latest
    container_name: lakehouse-statsd-exporter
    restart: unless-stopped
    command:
      - --statsd.listen-udp=:8125
      - --web.listen-address=:9102
    ports:
      - "9102:9102"
    networks:
      - lakehouse

  # ===========================================================================
  # Monitoring - Prometheus
  # ===========================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: lakehouse-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - prometheus-data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.retention.time=7d
      - --web.enable-lifecycle
    depends_on:
      - statsd-exporter
    networks:
      - lakehouse

  # ===========================================================================
  # Monitoring - Grafana
  # ===========================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: lakehouse-grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    networks:
      - lakehouse

  # ===========================================================================
  # Monitoring - PostgreSQL Exporter
  # ===========================================================================
  postgres-exporter:
    image: quay.io/prometheuscommunity/postgres-exporter:latest
    container_name: lakehouse-postgres-exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${POSTGRES_DB:-airflow}?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - lakehouse

  # ===========================================================================
  # Monitoring - Valkey/Redis Exporter
  # ===========================================================================
  valkey-exporter:
    image: oliver006/redis_exporter:latest
    container_name: lakehouse-valkey-exporter
    restart: unless-stopped
    environment:
      REDIS_ADDR: "redis://valkey:6379"
    ports:
      - "9121:9121"
    depends_on:
      valkey:
        condition: service_healthy
    networks:
      - lakehouse

# =============================================================================
# Volumes
# =============================================================================
volumes:
  postgres-data:
    driver: local
  valkey-data:
    driver: local
  seaweedfs-master-data:
    driver: local
  seaweedfs-volume-data:
    driver: local
  seaweedfs-filer-data:
    driver: local
  airflow-logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  lakehouse:
    name: lakehouse-network
    driver: bridge
